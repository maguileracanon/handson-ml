{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x=tf.Variable(3,name=\"x\")\n",
    "y=tf.Variable(4,name=\"y\")\n",
    "f=(x*x*y + y + 2)\n",
    "# Variables dont exist yet until a tensorflow session is initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer) # initialize variable\n",
    "sess.run(y.initializer)\n",
    "results = sess.run(f)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other option\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() # prepare an unit node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result= f.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# interactive session instead of normal seccion\n",
    "# doesnt need a with block because once is created is set to the default session\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result= f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1= tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple graphs\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2=tf.Variable(2)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so.. outside the with is not happening anymre\n",
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w=tf.constant(3)\n",
    "x= w+2\n",
    "y = x+5\n",
    "z= x*3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # evaluates x to evaluate y\n",
    "    print(z.eval()) # evaluate x again to evaluate z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# to avoid evaluating x and w twince\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y,z]) # evaluate both just in one graph \n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\maguileracanon\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing= fetch_california_housing()\n",
    "m,n= housing.data.shape # (shape of the dataset)\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)),housing.data] # why to add bias??\n",
    "\n",
    "X=tf.constant(housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y= tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")# -1 as parameter will insert the automatically detected shape\n",
    "XT= tf.transpose(X)\n",
    "theta= tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# compare with numpy\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate with gradient descent\n",
    "\n",
    "first we need to scale the data, because I am already familiarized with standardScaler I will implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408015\n",
      "Epoch 100 MSE = 0.75519705\n",
      "Epoch 200 MSE = 0.5420873\n",
      "Epoch 300 MSE = 0.53317\n",
      "Epoch 400 MSE = 0.53053814\n",
      "Epoch 500 MSE = 0.5287964\n",
      "Epoch 600 MSE = 0.52754897\n",
      "Epoch 700 MSE = 0.5266498\n",
      "Epoch 800 MSE = 0.5260011\n",
      "Epoch 900 MSE = 0.5255332\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X= tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32, name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32, name=\"y\")\n",
    "theta= tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")\n",
    "y_pred= tf.matmul(X,theta,name=\"predicitons\") \n",
    "error=y_pred - y\n",
    "mse= tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradients= tf.gradients(mse,[theta])[0]#gradients = (2/m) *(tf.matmul(tf.transpose(X),error))\n",
    "training_op = tf.assign(theta,theta - learning_rate*gradients) # this is the operation that will call other variables adn stuff\n",
    "# because they are nodes, I dont have to evaluate things over and avoer again :D, it kind of looks for the formula and evaluate as needed\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100 ==0: #each 100 iterations\n",
    "            print(\"Epoch\",epoch,\"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408015\n",
      "Epoch 100 MSE = 0.75519705\n",
      "Epoch 200 MSE = 0.5420873\n",
      "Epoch 300 MSE = 0.53317\n",
      "Epoch 400 MSE = 0.53053814\n",
      "Epoch 500 MSE = 0.5287964\n",
      "Epoch 600 MSE = 0.52754897\n",
      "Epoch 700 MSE = 0.5266498\n",
      "Epoch 800 MSE = 0.5260011\n",
      "Epoch 900 MSE = 0.5255332\n"
     ]
    }
   ],
   "source": [
    "# a better optimized solution\n",
    "reset_graph()\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "\n",
    "X= tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32, name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32, name=\"y\")\n",
    "theta= tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name=\"theta\")\n",
    "y_pred= tf.matmul(X,theta,name=\"predicitons\") \n",
    "error=y_pred - y\n",
    "mse= tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "##gradients= tf.gradients(mse,[theta])[0]#gradients = (2/m) *(tf.matmul(tf.transpose(X),error))\n",
    "#training_op = tf.assign(theta,theta - learning_rate*gradients) # this is the operation that will call other variables adn stuff\n",
    "optimizer= tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "# because they are nodes, I dont have to evaluate things over and avoer again :D, it kind of looks for the formula and evaluate as needed\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100 ==0: #each 100 iterations\n",
    "            print(\"Epoch\",epoch,\"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different optimizer is the momentum optimizer (that could converge much quicker)\n",
    "```python\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learningrate,momentum=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# placeholders to feed data\n",
    "\n",
    "A= tf.placeholder(tf.float32,shape=(None,3))\n",
    "B=A+5\n",
    "with tf.Session() as sess:\n",
    "    B_eval1=B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    B_eval2=B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})#Any number of rows\n",
    "    \n",
    "print(B_eval1)\n",
    "print(B_eval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make x and y placeholders\n",
    "X= tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "# define batch size\n",
    "batch_size = 100\n",
    "n_bathches= int(np.ceil(m/batch_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excecution phase\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_bathches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_bathches):\n",
    "            X_batch, y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op,feed_dict={X: X_batch, y:y_batch})\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161542\n",
      "Epoch 100 MSE = 0.7145004\n",
      "Epoch 200 MSE = 0.56670487\n",
      "Epoch 300 MSE = 0.5555718\n",
      "Epoch 400 MSE = 0.54881126\n",
      "Epoch 500 MSE = 0.5436363\n",
      "Epoch 600 MSE = 0.53962916\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.53406775\n",
      "Epoch 900 MSE = 0.5321473\n"
     ]
    }
   ],
   "source": [
    "# Saving\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Create server at the end of construction phase\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())        \n",
    "            # not shown\n",
    "            # call save whenever\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    # call save whenever\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restooring\n",
    "# Create seaver at the end of the construction phase\n",
    "# Call restores(instead of initializing variables) ath the beggining of the execution phase\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.3477088 ],\n",
       "       [ 0.36178365],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145283],\n",
       "       [-0.6375278 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saver saves and reestore all variables, but for more control one can only limit the scope, for example savel that will restore only the theta variable under the name weights:\n",
    "```python\n",
    "saver=tf.train.Saver({\"weights\":theta})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\" # A different log directory everytime the program is run or else Tensorboard will merge stats from different runs\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "init = tf.global_variables_initializer()\n",
    "# Create server at the end of construction phase\n",
    "#saver = tf.train.Saver()\n",
    "mse_summary=tf.summary.scalar('MSE',mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.0703337 ]\n",
      " [ 0.8637145 ]\n",
      " [ 0.12255152]\n",
      " [-0.31211877]\n",
      " [ 0.38510376]\n",
      " [ 0.00434168]\n",
      " [-0.0123295 ]\n",
      " [-0.83376896]\n",
      " [-0.8030471 ]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "#########################################################\n",
    "with tf.name_scope(\"loss\")as scope:\n",
    "    error = y_pred - y\n",
    "    mse= tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that does something, it is not important what right now\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z= tf.add(tf.matmul(X,w),b,name=\"z\") # I am assuming all of these names are for the tensorgraph\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features),name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus,name=\"output\") # add all results from 1 function\n",
    "\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu1\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sharing variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\",reuse=True):\n",
    "        threshold=tf.get_variable(\"threshold\") # reuse existing variable\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z= tf.add(tf.matmul(X,w),b,name=\"z\") # I am assuming all of these names are for the tensorgraph\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features),name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0)) # creates\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus,name=\"output\") # add all results from 1 function\n",
    "\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu1\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative so that the threshold variable doesnt have to be assigned outside of the relu funciton\n",
    "reset_graph()\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0)) # reuse existing variable\n",
    "    w_shape=(int(X.get_shape()[1]),1)\n",
    "    w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "    b=tf.Variable(0.0,name=\"bias\")\n",
    "    z= tf.add(tf.matmul(X,w),b,name=\"z\") # I am assuming all of these names are for the tensorgraph\n",
    "    return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features),name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\",reuse=(relu_index>=1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus,name=\"output\") # add all results from 1 function\n",
    "\n",
    "file_writer=tf.summary.FileWriter(\"logs/relu1\",tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
